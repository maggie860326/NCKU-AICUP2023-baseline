{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline\n",
        "python: 3.8.*\n",
        "\n",
        "use ```Ctrl + ]``` to collapse all section :)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download our starter pack (3~5 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: gdown==4.7.1 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from -r requirements.txt (line 2)) (4.7.1)\n",
            "Requirement already satisfied: ipywidgets==8.0.5 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from -r requirements.txt (line 3)) (8.0.5)\n",
            "Requirement already satisfied: pandarallel==1.6.4 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from -r requirements.txt (line 4)) (1.6.4)\n",
            "Requirement already satisfied: pandas==1.5.3 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from -r requirements.txt (line 5)) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from -r requirements.txt (line 6)) (1.2.2)\n",
            "Requirement already satisfied: tensorboard==2.11.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from -r requirements.txt (line 7)) (2.11.0)\n",
            "Requirement already satisfied: torch==1.13.1+cu116 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from -r requirements.txt (line 8)) (1.13.1+cu116)\n",
            "Requirement already satisfied: tqdm==4.65.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from -r requirements.txt (line 9)) (4.65.0)\n",
            "Requirement already satisfied: opencc in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from -r requirements.txt (line 10)) (1.1.1)\n",
            "Requirement already satisfied: hanlp in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from -r requirements.txt (line 11)) (2.1.0b49)\n",
            "Requirement already satisfied: transformers==4.27.1 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from -r requirements.txt (line 12)) (4.27.1)\n",
            "Requirement already satisfied: wikipedia==1.4.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from -r requirements.txt (line 13)) (1.4.0)\n",
            "Requirement already satisfied: six in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from gdown==4.7.1->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from gdown==4.7.1->-r requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from gdown==4.7.1->-r requirements.txt (line 2)) (3.12.0)\n",
            "Requirement already satisfied: requests[socks] in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from gdown==4.7.1->-r requirements.txt (line 2)) (2.30.0)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from ipywidgets==8.0.5->-r requirements.txt (line 3)) (4.0.7)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from ipywidgets==8.0.5->-r requirements.txt (line 3)) (3.0.7)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from ipywidgets==8.0.5->-r requirements.txt (line 3)) (5.9.0)\n",
            "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from ipywidgets==8.0.5->-r requirements.txt (line 3)) (8.4.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from pandarallel==1.6.4->-r requirements.txt (line 4)) (5.9.0)\n",
            "Requirement already satisfied: dill>=0.3.1 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from pandarallel==1.6.4->-r requirements.txt (line 4)) (0.3.6)\n",
            "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from pandas==1.5.3->-r requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from pandas==1.5.3->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from pandas==1.5.3->-r requirements.txt (line 5)) (2023.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 6)) (1.10.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (2.3.4)\n",
            "Requirement already satisfied: wheel>=0.26 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (0.38.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (66.0.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.9.2 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (3.20.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (1.54.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from tensorboard==2.11.0->-r requirements.txt (line 7)) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from torch==1.13.1+cu116->-r requirements.txt (line 8)) (4.5.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from tqdm==4.65.0->-r requirements.txt (line 9)) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from transformers==4.27.1->-r requirements.txt (line 12)) (2023.5.5)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from transformers==4.27.1->-r requirements.txt (line 12)) (0.11.6)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from transformers==4.27.1->-r requirements.txt (line 12)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from transformers==4.27.1->-r requirements.txt (line 12)) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from transformers==4.27.1->-r requirements.txt (line 12)) (0.14.1)\n",
            "Requirement already satisfied: toposort==1.5 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from hanlp->-r requirements.txt (line 11)) (1.5)\n",
            "Requirement already satisfied: hanlp-trie>=0.0.4 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from hanlp->-r requirements.txt (line 11)) (0.0.5)\n",
            "Requirement already satisfied: hanlp-downloader in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from hanlp->-r requirements.txt (line 11)) (0.0.25)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from hanlp->-r requirements.txt (line 11)) (0.1.99)\n",
            "Requirement already satisfied: hanlp-common>=0.0.19 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from hanlp->-r requirements.txt (line 11)) (0.0.19)\n",
            "Requirement already satisfied: pynvml in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from hanlp->-r requirements.txt (line 11)) (11.5.0)\n",
            "Requirement already satisfied: termcolor in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from hanlp->-r requirements.txt (line 11)) (2.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard==2.11.0->-r requirements.txt (line 7)) (0.3.0)\n",
            "Requirement already satisfied: urllib3<2.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard==2.11.0->-r requirements.txt (line 7)) (1.26.15)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard==2.11.0->-r requirements.txt (line 7)) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard==2.11.0->-r requirements.txt (line 7)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.11.0->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: phrasetree in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from hanlp-common>=0.0.19->hanlp->-r requirements.txt (line 11)) (0.0.8)\n",
            "Requirement already satisfied: fsspec in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.1->-r requirements.txt (line 12)) (2023.5.0)\n",
            "Requirement already satisfied: matplotlib-inline in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (0.1.6)\n",
            "Requirement already satisfied: pickleshare in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (0.7.5)\n",
            "Requirement already satisfied: stack-data in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (0.6.2)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (3.0.38)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (2.15.1)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (0.18.2)\n",
            "Requirement already satisfied: decorator in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (5.1.1)\n",
            "Requirement already satisfied: backcall in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from markdown>=2.6.8->tensorboard==2.11.0->-r requirements.txt (line 7)) (6.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from requests[socks]->gdown==4.7.1->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from requests[socks]->gdown==4.7.1->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from requests[socks]->gdown==4.7.1->-r requirements.txt (line 2)) (2023.5.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard==2.11.0->-r requirements.txt (line 7)) (2.1.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from beautifulsoup4->gdown==4.7.1->-r requirements.txt (line 2)) (2.4.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from requests[socks]->gdown==4.7.1->-r requirements.txt (line 2)) (1.7.1)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.11.0->-r requirements.txt (line 7)) (3.15.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (0.2.6)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.11.0->-r requirements.txt (line 7)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.11.0->-r requirements.txt (line 7)) (3.2.2)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (0.2.2)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (2.2.1)\n",
            "Requirement already satisfied: executing>=1.2.0 in c:\\users\\maggie\\miniconda3\\envs\\nlp_final\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.0.5->-r requirements.txt (line 3)) (1.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# 新增 gdown 到 requirements.txt\n",
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\dev_doc5sent5.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\dev_doc5sent5.pkl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\hanlp_con_results.pkl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\hanlp_con_test_results.pkl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\public_test.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\public_train.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\test_doc5.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\test_doc5sent5.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\test_doc5sent5.pkl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\train_doc5.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\train_doc5sent5.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\train_doc5sent5.pkl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-001.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-002.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-003.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-004.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-005.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-006.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-007.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-008.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-009.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-010.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-011.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-012.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-013.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-014.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-015.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-016.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-017.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-018.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-019.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-020.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-021.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-022.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-023.jsonl',\n",
              " 'c:\\\\Users\\\\Maggie\\\\Documents\\\\Git_Workspace\\\\2023_spring_NLP\\\\NLP_final_project\\\\NCKU-AICUP2023-baseline\\\\data\\\\wiki-pages\\\\wiki-024.jsonl']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 從 google drive 下載 data 資料夾\n",
        "import gdown\n",
        "url = \"https://drive.google.com/drive/folders/1r742uPeVnqUm04qUEZpzzpxXZFbj4gNd\"\n",
        "gdown.download_folder(url, quiet=True, use_cookies=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 透過 command line 下載\n",
        "# !gdown --folder 1r742uPeVnqUm04qUEZpzzpxXZFbj4gNd"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h7MSEcenjVrL"
      },
      "source": [
        "notebook1\n",
        "## PART 1. Document retrieval"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare the environment and import all library we need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "niqu9pLajYC_"
      },
      "outputs": [],
      "source": [
        "# built-in libs\n",
        "import json\n",
        "import pickle\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Set, Tuple, Union\n",
        "\n",
        "# 3rd party libs\n",
        "import hanlp\n",
        "import opencc\n",
        "import pandas as pd\n",
        "import wikipedia\n",
        "from hanlp.components.pipeline import Pipeline\n",
        "from pandarallel import pandarallel\n",
        "\n",
        "# our own libs\n",
        "from utils import load_json\n",
        "\n",
        "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)\n",
        "wikipedia.set_lang(\"zh\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preload the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_DATA = load_json(\"data/public_train.jsonl\")\n",
        "TEST_DATA = load_json(\"data/public_test.jsonl\")\n",
        "CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
        "CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data class for type hinting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Claim:\n",
        "    data: str\n",
        "\n",
        "@dataclass\n",
        "class AnnotationID:\n",
        "    id: int\n",
        "\n",
        "@dataclass\n",
        "class EvidenceID:\n",
        "    id: int\n",
        "\n",
        "@dataclass\n",
        "class PageTitle:\n",
        "    title: str\n",
        "\n",
        "@dataclass\n",
        "class SentenceID:\n",
        "    id: int\n",
        "\n",
        "@dataclass\n",
        "class Evidence:\n",
        "    data: List[List[Tuple[AnnotationID, EvidenceID, PageTitle, SentenceID]]]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper function"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the sake of consistency, we convert traditional to simplified Chinese first before converting it back to traditional Chinese.  This is due to some errors occuring when converting traditional to traditional Chinese."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "A3NU01DnjKp-"
      },
      "outputs": [],
      "source": [
        "# 繁體轉簡體再轉繁體\n",
        "def do_st_corrections(text: str) -> str:\n",
        "    simplified = CONVERTER_T2S.convert(text)\n",
        "\n",
        "    return CONVERTER_S2T.convert(simplified)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use constituency parsing to separate part of speeches or so called constituent to extract noun phrases.  In the later stages, we will use the noun phrases as the query to search for relevant documents.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 把claim丟進predictor做pipeline取 noun phrase\n",
        "def get_nps_hanlp(\n",
        "    predictor: Pipeline,\n",
        "    d: Dict[str, Union[int, Claim, Evidence]],\n",
        ") -> List[str]:\n",
        "    claim = d[\"claim\"]\n",
        "    tree = predictor(claim)[\"con\"]\n",
        "    nps = [\n",
        "        do_st_corrections(\"\".join(subtree.leaves()))\n",
        "        for subtree in tree.subtrees(lambda t: t.label() == \"NP\")\n",
        "    ]\n",
        "\n",
        "    return nps"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Precision refers to how many related documents are retrieved.  Recall refers to how many relevant documents are retrieved.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 選出的有多少比例正確\n",
        "def calculate_precision(\n",
        "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
        "    predictions: pd.Series,\n",
        ") -> None:\n",
        "    precision = 0\n",
        "    count = 0\n",
        "\n",
        "    for i, d in enumerate(data):\n",
        "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
        "            continue\n",
        "\n",
        "        # Extract all ground truth of titles of the wikipedia pages\n",
        "        # evidence[2] refers to the title of the wikipedia page\n",
        "        gt_pages = set([\n",
        "            evidence[2]\n",
        "            for evidence_set in d[\"evidence\"]\n",
        "            for evidence in evidence_set\n",
        "        ])\n",
        "\n",
        "        predicted_pages = predictions.iloc[i]\n",
        "        hits = predicted_pages.intersection(gt_pages)\n",
        "        if len(predicted_pages) != 0:\n",
        "            precision += len(hits) / len(predicted_pages)\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    # Macro precision\n",
        "    print(f\"Precision: {precision / count}\")\n",
        "\n",
        "# 正確中有多少被我們取出\n",
        "def calculate_recall(\n",
        "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
        "    predictions: pd.Series,\n",
        ") -> None:\n",
        "    recall = 0\n",
        "    count = 0\n",
        "\n",
        "    for i, d in enumerate(data):\n",
        "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
        "            continue\n",
        "\n",
        "        gt_pages = set([\n",
        "            evidence[2]\n",
        "            for evidence_set in d[\"evidence\"]\n",
        "            for evidence in evidence_set\n",
        "        ])\n",
        "        predicted_pages = predictions.iloc[i]\n",
        "        hits = predicted_pages.intersection(gt_pages)\n",
        "        recall += len(hits) / len(gt_pages)\n",
        "        count += 1\n",
        "\n",
        "    print(f\"Recall: {recall / count}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The default amount of documents retrieved is at most five documents.  This `num_pred_doc` can be adjusted based on your objective.  Save data in jsonl format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 把選出的前五個 wiki 頁面存起來\n",
        "def save_doc(\n",
        "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
        "    predictions: pd.Series,\n",
        "    mode: str = \"train\",\n",
        "    num_pred_doc: int = 5,\n",
        ") -> None:\n",
        "    with open(\n",
        "        f\"data/{mode}_doc{num_pred_doc}.jsonl\",\n",
        "        \"w\",\n",
        "        encoding=\"utf8\",\n",
        "    ) as f:\n",
        "        for i, d in enumerate(data):\n",
        "            d[\"predicted_pages\"] = list(predictions.iloc[i])\n",
        "            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main function for document retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ayGI44qkk_wy"
      },
      "outputs": [],
      "source": [
        "# 取得相關的前五個wiki頁面\n",
        "def get_pred_pages(series_data: pd.Series) -> Set[Dict[int, str]]:\n",
        "    results = []\n",
        "    tmp_muji = []\n",
        "    # wiki_page: its index showned in claim\n",
        "    mapping = {}\n",
        "    claim = series_data[\"claim\"]\n",
        "    nps = series_data[\"hanlp_results\"]\n",
        "    first_wiki_term = []\n",
        "\n",
        "    for i, np in enumerate(nps):\n",
        "        # Simplified Traditional Chinese Correction\n",
        "        wiki_search_results = [\n",
        "            do_st_corrections(w) for w in wikipedia.search(np) # wiki API 可以找出與關鍵詞有關的頁面名稱(按相關度排序)\n",
        "        ]\n",
        "\n",
        "        # Remove the wiki page's description in brackets 去除後綴，例如\"()\"\n",
        "        wiki_set = [re.sub(r\"\\s\\(\\S+\\)\", \"\", w) for w in wiki_search_results]\n",
        "        wiki_df = pd.DataFrame({\n",
        "            \"wiki_set\": wiki_set, # 去掉後綴的頁面\n",
        "            \"wiki_results\": wiki_search_results # 沒有去後綴的\n",
        "        })\n",
        "\n",
        "        # Elements in wiki_set --> index\n",
        "        # Extracting only the first element is one way to avoid extracting\n",
        "        # too many of the similar wiki pages\n",
        "        grouped_df = wiki_df.groupby(\"wiki_set\", sort=False).first() #把去掉後綴後相同的頁面group by在一起\n",
        "        candidates = grouped_df[\"wiki_results\"].tolist() # 沒有去掉後綴的頁面list\n",
        "        # muji refers to wiki_set\n",
        "        muji = grouped_df.index.tolist()  # 把去掉後綴後相同的頁面group by在一起之後轉成list\n",
        "\n",
        "        # \n",
        "        for prefix, term in zip(muji, candidates):\n",
        "            if prefix not in tmp_muji: # 表示這個muji還沒做過\n",
        "                matched = False\n",
        "\n",
        "                # Take at least one term from the first noun phrase\n",
        "                if i == 0:\n",
        "                    first_wiki_term.append(term)\n",
        "\n",
        "                # Walrus operator :=\n",
        "                # https://docs.python.org/3/whatsnew/3.8.html#assignment-expressions\n",
        "                # Through these filters, we are trying to figure out if the term\n",
        "                # is within the claim\n",
        "                # 如果new_term在claim裡面，則令new_term=term\n",
        "                if (((new_term := term) in claim) or                    \n",
        "                    # 如果new_term 在 replace \".\" with \"\" 之後在claim裡面，則令new_term=term\n",
        "                    ((new_term := term.replace(\"·\", \"\")) in claim) or\n",
        "                    ((new_term := term.split(\" \")[0]) in claim) or\n",
        "                    ((new_term := term.replace(\"-\", \" \")) in claim)):\n",
        "                    matched = True # 表示有對應到 claim\n",
        "\n",
        "                elif \"·\" in term: # 處理外國人名中間的點\n",
        "                    splitted = term.split(\"·\")\n",
        "                    for split in splitted:\n",
        "                        if (new_term := split) in claim:\n",
        "                            matched = True\n",
        "                            break\n",
        "\n",
        "                if matched: # 如果 new_term 有在 claim 中\n",
        "                    # post-processing\n",
        "                    term = term.replace(\" \", \"_\")\n",
        "                    term = term.replace(\"-\", \"\")\n",
        "                    results.append(term)\n",
        "                    mapping[term] = claim.find(new_term) # 字典 {term:在claim中的位置}，\"天王星\":6\n",
        "                    tmp_muji.append(new_term) # list\n",
        "\n",
        "    # 5 is a hyperparameter\n",
        "    if len(results) > 5:\n",
        "        assert -1 not in mapping.values()\n",
        "        results = sorted(mapping, key=mapping.get)[:5] # 選出前五個 wiki 頁面\n",
        "    elif len(results) < 1:\n",
        "        results = first_wiki_term\n",
        "\n",
        "    return set(results) # 回傳前五筆相關的wiki頁面"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1. Get noun phrases from hanlp consituency parsing tree"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup [HanLP](https://github.com/hankcs/HanLP) predictor (1 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://file.hankcs.com/hanlp/tok/fine_electra_small_20220615_231803.zip to C:\\Users\\Maggie\\AppData\\Roaming\\hanlp\\tok/fine_electra_small_20220615_231803.zip\n",
            "100%  43.5 MiB  23.0 KiB/s ETA:  0 s [=========================================]\n",
            "Decompressing C:\\Users\\Maggie\\AppData\\Roaming\\hanlp\\tok/fine_electra_small_20220615_231803.zip to C:\\Users\\Maggie\\AppData\\Roaming\\hanlp\\tok\n",
            "Downloading https://file.hankcs.com/hanlp/utils/char_table_20210602_202632.json.zip to C:\\Users\\Maggie\\AppData\\Roaming\\hanlp\\utils/char_table_20210602_202632.json.zip\n",
            "100%  26.7 KiB  26.7 KiB/s ETA:  0 s [=========================================]\n",
            "Decompressing C:\\Users\\Maggie\\AppData\\Roaming\\hanlp\\utils/char_table_20210602_202632.json.zip to C:\\Users\\Maggie\\AppData\\Roaming\\hanlp\\utils\n",
            "Downloading https://file.hankcs.com/hanlp/transformers/electra_zh_small_20210706_125427.zip to C:\\Users\\Maggie\\AppData\\Roaming\\hanlp\\transformers/electra_zh_small_20210706_125427.zip\n",
            "100%  41.2 KiB  41.2 KiB/s ETA:  0 s [=========================================]\n",
            "Decompressing C:\\Users\\Maggie\\AppData\\Roaming\\hanlp\\transformers/electra_zh_small_20210706_125427.zip to C:\\Users\\Maggie\\AppData\\Roaming\\hanlp\\transformers\n",
            "Downloading https://file.hankcs.com/hanlp/constituency/ctb9_con_electra_small_20220215_230116.zip to C:\\Users\\Maggie\\AppData\\Roaming\\hanlp\\constituency/ctb9_con_electra_small_20220215_230116.zip\n",
            "100%  54.8 MiB   5.2 KiB/s ETA:  0 s [=========================================]\n",
            "Decompressing C:\\Users\\Maggie\\AppData\\Roaming\\hanlp\\constituency/ctb9_con_electra_small_20220215_230116.zip to C:\\Users\\Maggie\\AppData\\Roaming\\hanlp\\constituency\n",
            "                                   \r"
          ]
        }
      ],
      "source": [
        "# predictor裡面會包含一系列動作\n",
        "predictor = (hanlp.pipeline().append(\n",
        "    hanlp.load(\"FINE_ELECTRA_SMALL_ZH\"),\n",
        "    output_key=\"tok\",\n",
        ").append(\n",
        "    hanlp.load(\"CTB9_CON_ELECTRA_SMALL\"),\n",
        "    output_key=\"con\",\n",
        "    input_key=\"tok\",\n",
        "))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will skip this process which for creating parsing tree when demo on class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "hanlp_file = f\"data/hanlp_con_results.pkl\"\n",
        "if Path(hanlp_file).exists():\n",
        "    with open(hanlp_file, \"rb\") as f:\n",
        "        hanlp_results = pickle.load(f)\n",
        "else:\n",
        "    hanlp_results = [get_nps_hanlp(predictor, d) for d in TRAIN_DATA]\n",
        "    with open(hanlp_file, \"wb\") as f:\n",
        "        pickle.dump(hanlp_results, f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get pages via wiki online api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_path = f\"data/train_doc5.jsonl\"\n",
        "if Path(doc_path).exists():\n",
        "    with open(doc_path, \"r\", encoding=\"utf8\") as f:\n",
        "        predicted_results = pd.Series([\n",
        "            set(json.loads(line)[\"predicted_pages\"])\n",
        "            for line in f\n",
        "        ])\n",
        "else:\n",
        "    train_df = pd.DataFrame(TRAIN_DATA)\n",
        "    train_df.loc[:, \"hanlp_results\"] = hanlp_results\n",
        "    predicted_results = train_df.parallel_apply(get_pred_pages, axis=1) # 取得每個train data的前五相關wiki頁面\n",
        "    save_doc(TRAIN_DATA, predicted_results, mode=\"train\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2. Calculate our results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.2509375000000056\n",
            "Recall: 0.8073333333333337\n"
          ]
        }
      ],
      "source": [
        "calculate_precision(TRAIN_DATA, predicted_results)\n",
        "calculate_recall(TRAIN_DATA, predicted_results)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3. Repeat the same process on test set\n",
        "Create parsing tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "hanlp_test_file = f\"data/hanlp_con_test_results.pkl\"\n",
        "if Path(hanlp_test_file).exists():\n",
        "    with open(hanlp_file, \"rb\") as f:\n",
        "        hanlp_results = pickle.load(f)\n",
        "else:\n",
        "    hanlp_results = [get_nps_hanlp(predictor, d) for d in TEST_DATA]\n",
        "    with open(hanlp_file, \"wb\") as f:\n",
        "        pickle.dump(hanlp_results, f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get pages via wiki online api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_doc_path = f\"data/test_doc5.jsonl\"\n",
        "if Path(test_doc_path).exists():\n",
        "    with open(test_doc_path, \"r\", encoding=\"utf8\") as f:\n",
        "        test_results = pd.Series(\n",
        "            [set(json.loads(line)[\"predicted_pages\"]) for line in f])\n",
        "else:\n",
        "    test_df = pd.DataFrame(TEST_DATA)\n",
        "    test_df.loc[:, \"hanlp_results\"] = hanlp_results\n",
        "    test_results = test_df.parallel_apply(get_pred_pages, axis=1)\n",
        "    save_doc(TEST_DATA, test_results, mode=\"test\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol4zFkSbjgXF"
      },
      "source": [
        "\n",
        "## PART 2. Sentence retrieval"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import some libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "GlliDsgXjisj"
      },
      "outputs": [],
      "source": [
        "# built-in libs\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Set, Tuple, Union\n",
        "\n",
        "# third-party libs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandarallel import pandarallel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    get_scheduler,\n",
        ")\n",
        "\n",
        "from dataset import BERTDataset, Dataset\n",
        "\n",
        "# local libs\n",
        "from utils import (\n",
        "    generate_evidence_to_wiki_pages_mapping,\n",
        "    jsonl_dir_to_df,\n",
        "    load_json,\n",
        "    load_model,\n",
        "    save_checkpoint,\n",
        "    set_lr_scheduler,\n",
        ")\n",
        "\n",
        "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Global variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "J3BBLE3_hlPi"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "\n",
        "TRAIN_DATA = load_json(\"data/public_train.jsonl\")\n",
        "TEST_DATA = load_json(\"data/public_test.jsonl\")\n",
        "DOC_DATA = load_json(\"data/train_doc5.jsonl\")\n",
        "\n",
        "LABEL2ID: Dict[str, int] = {\n",
        "    \"supports\": 0,\n",
        "    \"refutes\": 1,\n",
        "    \"NOT ENOUGH INFO\": 2,\n",
        "}\n",
        "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
        "\n",
        "_y = [LABEL2ID[data[\"label\"]] for data in TRAIN_DATA]\n",
        "# GT means Ground Truth\n",
        "TRAIN_GT, DEV_GT = train_test_split(\n",
        "    DOC_DATA,\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        "    shuffle=True,\n",
        "    stratify=_y,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preload wiki database (1 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading and concatenating jsonl files in data/wiki-pages\n",
            "Generate parse mapping\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5a6ee3d87fd462ca3f28853e143ae15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=118776), Label(value='0 / 118776')…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transform to id to evidence_map mapping\n"
          ]
        }
      ],
      "source": [
        "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
        "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages)\n",
        "del wiki_pages"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper function"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate precision for sentence retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evidence_macro_precision(\n",
        "    instance: Dict,\n",
        "    top_rows: pd.DataFrame,\n",
        ") -> Tuple[float, float]:\n",
        "    \n",
        "    \"\"\"Calculate precision for sentence retrieval\n",
        "    This function is modified from fever-scorer.\n",
        "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
        "\n",
        "    Args:\n",
        "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
        "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
        "\n",
        "        IMPORTANT!!!\n",
        "        instance (dict) should have the key of `evidence`.\n",
        "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]:\n",
        "        [1]: relevant and retrieved (numerator of precision)\n",
        "        [2]: retrieved (denominator of precision)\n",
        "    \"\"\"\n",
        "    this_precision = 0.0\n",
        "    this_precision_hits = 0.0\n",
        "\n",
        "    # Return 0, 0 if label is not enough info since not enough info does not\n",
        "    # contain any evidence.\n",
        "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
        "        # e[2] is the page title, e[3] is the sentence index\n",
        "        all_evi = [[e[2], e[3]]\n",
        "                   for eg in instance[\"evidence\"]\n",
        "                   for e in eg\n",
        "                   if e[3] is not None]\n",
        "        claim = instance[\"claim\"]\n",
        "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
        "                                      claim][\"predicted_evidence\"].tolist()\n",
        "\n",
        "        for prediction in predicted_evidence:\n",
        "            if prediction in all_evi:\n",
        "                this_precision += 1.0\n",
        "            this_precision_hits += 1.0\n",
        "\n",
        "        return (this_precision /\n",
        "                this_precision_hits) if this_precision_hits > 0 else 1.0, 1.0\n",
        "\n",
        "    return 0.0, 0.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate recall for sentence retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evidence_macro_recall(\n",
        "    instance: Dict,\n",
        "    top_rows: pd.DataFrame,\n",
        ") -> Tuple[float, float]:\n",
        "    \n",
        "    \"\"\"Calculate recall for sentence retrieval\n",
        "    This function is modified from fever-scorer.\n",
        "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
        "\n",
        "    Args:\n",
        "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
        "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
        "\n",
        "        IMPORTANT!!!\n",
        "        instance (dict) should have the key of `evidence`.\n",
        "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]:\n",
        "        [1]: relevant and retrieved (numerator of recall)\n",
        "        [2]: relevant (denominator of recall)\n",
        "    \"\"\"\n",
        "    # We only want to score F1/Precision/Recall of recalled evidence for NEI claims\n",
        "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
        "        # If there's no evidence to predict, return 1\n",
        "        if len(instance[\"evidence\"]) == 0 or all(\n",
        "            [len(eg) == 0 for eg in instance]):\n",
        "            return 1.0, 1.0\n",
        "\n",
        "        claim = instance[\"claim\"]\n",
        "\n",
        "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
        "                                      claim][\"predicted_evidence\"].tolist()\n",
        "\n",
        "        for evidence_group in instance[\"evidence\"]:\n",
        "            evidence = [[e[2], e[3]] for e in evidence_group]\n",
        "            if all([item in predicted_evidence for item in evidence]):\n",
        "                # We only want to score complete groups of evidence. Incomplete\n",
        "                # groups are worthless.\n",
        "                return 1.0, 1.0\n",
        "        return 0.0, 1.0\n",
        "    return 0.0, 0.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the scores of sentence retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_retrieval(\n",
        "    probs: np.ndarray,\n",
        "    df_evidences: pd.DataFrame,\n",
        "    ground_truths: pd.DataFrame,\n",
        "    top_n: int = 5,\n",
        "    cal_scores: bool = True,\n",
        "    save_name: str = None,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"Calculate the scores of sentence retrieval\n",
        "\n",
        "    Args:\n",
        "        probs (np.ndarray): probabilities of the candidate retrieved sentences\n",
        "        df_evidences (pd.DataFrame): the candiate evidence sentences paired with claims\n",
        "        ground_truths (pd.DataFrame): the loaded data of dev.jsonl or test.jsonl\n",
        "        top_n (int, optional): the number of the retrieved sentences. Defaults to 2.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: F1 score, precision, and recall\n",
        "    \"\"\"\n",
        "    df_evidences[\"prob\"] = probs\n",
        "    top_rows = (\n",
        "        df_evidences.groupby(\"claim\").apply(\n",
        "        lambda x: x.nlargest(top_n, \"prob\"))\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    if cal_scores:\n",
        "        macro_precision = 0\n",
        "        macro_precision_hits = 0\n",
        "        macro_recall = 0\n",
        "        macro_recall_hits = 0\n",
        "\n",
        "        for i, instance in enumerate(ground_truths):\n",
        "            macro_prec = evidence_macro_precision(instance, top_rows)\n",
        "            macro_precision += macro_prec[0]\n",
        "            macro_precision_hits += macro_prec[1]\n",
        "\n",
        "            macro_rec = evidence_macro_recall(instance, top_rows)\n",
        "            macro_recall += macro_rec[0]\n",
        "            macro_recall_hits += macro_rec[1]\n",
        "\n",
        "        pr = (macro_precision /\n",
        "              macro_precision_hits) if macro_precision_hits > 0 else 1.0\n",
        "        rec = (macro_recall /\n",
        "               macro_recall_hits) if macro_recall_hits > 0 else 0.0\n",
        "        f1 = 2.0 * pr * rec / (pr + rec)\n",
        "\n",
        "    if save_name is not None:\n",
        "        # write doc7_sent5 file\n",
        "        with open(f\"data/{save_name}\", \"w\") as f:\n",
        "            for instance in ground_truths:\n",
        "                claim = instance[\"claim\"]\n",
        "                predicted_evidence = top_rows[\n",
        "                    top_rows[\"claim\"] == claim][\"predicted_evidence\"].tolist()\n",
        "                instance[\"predicted_evidence\"] = predicted_evidence\n",
        "                f.write(json.dumps(instance, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    if cal_scores:\n",
        "        return {\"F1 score\": f1, \"Precision\": pr, \"Recall\": rec}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inference script to get probabilites for the candidate evidence sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 計算positive的機率\n",
        "def get_predicted_probs(\n",
        "    model: nn.Module,\n",
        "    dataloader: Dataset,\n",
        "    device: torch.device,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Inference script to get probabilites for the candidate evidence sentences\n",
        "\n",
        "    Args:\n",
        "        model: the one from HuggingFace Transformers\n",
        "        dataloader: devset or testset in torch dataloader\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: probabilites of the candidate evidence sentences\n",
        "    \"\"\"\n",
        "    model.eval() # 開啟 evaluation 模式\n",
        "    probs = []\n",
        "\n",
        "    with torch.no_grad(): # 不要做 gradient 計算\n",
        "        for batch in tqdm(dataloader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            # 算出機率後只取 positive 的機率\n",
        "            probs.extend(torch.softmax(logits, dim=1)[:, 1].tolist())\n",
        "\n",
        "    return np.array(probs) # 回傳 positive 的機率"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AicupTopkEvidenceBERTDataset class for AICUP dataset with top-k evidence sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SentRetrievalBERTDataset(BERTDataset):\n",
        "    \"\"\"AicupTopkEvidenceBERTDataset class for AICUP dataset with top-k evidence sentences.\"\"\"\n",
        "\n",
        "    def __getitem__( \n",
        "        self,\n",
        "        idx: int,\n",
        "        **kwargs,\n",
        "    ) -> Tuple[Dict[str, torch.Tensor], int]: # 回傳經過 torch.tokenizer的 claim, wiki相關句, label\n",
        "        item = self.data.iloc[idx]\n",
        "        sentA = item[\"claim\"] # claim\n",
        "        sentB = item[\"text\"] # 用空格串起的句子們\n",
        "\n",
        "        # claim [SEP] text\n",
        "        concat = self.tokenizer(\n",
        "            sentA, # claim\n",
        "            sentB, # 用空格串起的句子們\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        # 轉成 tensor\n",
        "        concat_ten = {k: torch.tensor(v) for k, v in concat.items()}\n",
        "        if \"label\" in item:\n",
        "            concat_ten[\"labels\"] = torch.tensor(item[\"label\"]) # 再加上 label 的 tensor\n",
        "\n",
        "        return concat_ten"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main function for sentence retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "gpvXpFwXszfv"
      },
      "outputs": [],
      "source": [
        "# pair_with_wiki_sentences 把wiki_pages資料夾內的資料整理成一個字典\n",
        "\n",
        "def pair_with_wiki_sentences(\n",
        "    mapping: Dict[str, Dict[int, str]], # 就是把wiki_pages資料夾內的資料整理成一個字典{頁面名稱: {index: 句子}}\n",
        "    df: pd.DataFrame,\n",
        "    negative_ratio: float,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Only for creating train sentences.\"\"\"\n",
        "    claims = []\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    # positive 表示有出現在 training data 裡面\n",
        "    for i in range(len(df)):\n",
        "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
        "            continue # 如果NOT ENOUGH INFO就跳過\n",
        "\n",
        "        claim = df[\"claim\"].iloc[i]\n",
        "        evidence_sets = df[\"evidence\"].iloc[i] # [[[6635,6392,'真靈長大目',3]]]\n",
        "        for evidence_set in evidence_sets: # 針對每個\n",
        "            sents = []\n",
        "            for evidence in evidence_set: # 針對每個句子\n",
        "                # evidence[2] is the page title\n",
        "                page = evidence[2].replace(\" \", \"_\") # 把空格替換成底線\n",
        "                # the only page with weird name 特殊例外處理的句子\n",
        "                if page == \"臺灣海峽危機#第二次臺灣海峽危機（1958）\":\n",
        "                    continue\n",
        "                # evidence[3] is in form of int however, mapping requires str\n",
        "                sent_idx = str(evidence[3]) # 取出句子的index\n",
        "                sents.append(mapping[page][sent_idx]) # 將該句子加到sents list中\n",
        "\n",
        "            whole_evidence = \" \".join(sents) # 把所有sents list的句子用空格接起來\n",
        "\n",
        "\n",
        "            claims.append(claim) # 把該claim加到list\n",
        "            sentences.append(whole_evidence) # 將用空格串連的證據句加到list\n",
        "            labels.append(1) # 表示 positive\n",
        "\n",
        "    # negative 表示沒有出現在 training data 裡面\n",
        "    for i in range(len(df)):\n",
        "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
        "            continue\n",
        "        claim = df[\"claim\"].iloc[i]\n",
        "\n",
        "        evidence_set = set([(evidence[2], evidence[3])\n",
        "                            for evidences in  df[\"evidence\"][i]\n",
        "                            for evidence in evidences])\n",
        "        predicted_pages = df[\"predicted_pages\"][i] # 第一階段最後的產出，wiki 頁面前五筆的index和頁面\n",
        "        for page in predicted_pages: # 針對每個 wiki 頁面 \n",
        "            page = page.replace(\" \", \"_\")\n",
        "            try:\n",
        "                page_sent_id_pairs = [ # list [頁面:句子index]\n",
        "                    (page, sent_idx) for sent_idx in mapping[page].keys()\n",
        "                ]\n",
        "            except KeyError:\n",
        "                # print(f\"{page} is not in our Wiki db.\")\n",
        "                continue\n",
        "\n",
        "            for pair in page_sent_id_pairs: # 表示有在training data裡\n",
        "                if pair in evidence_set:\n",
        "                    continue\n",
        "                text = mapping[page][pair[1]]\n",
        "                # `np.random.rand(1) <= 0.05`: Control not to add too many negative samples\n",
        "                if text != \"\" and np.random.rand(1) <= negative_ratio: # 為了避免 positive 與 negative 資料數量不平衡，所以用threshold控制\n",
        "                    claims.append(claim)\n",
        "                    sentences.append(text)\n",
        "                    labels.append(0)\n",
        "    # 回傳 {claims, 句子空格串聯, labels}\n",
        "    return pd.DataFrame({\"claim\": claims, \"text\": sentences, \"label\": labels})\n",
        "\n",
        "\n",
        "def pair_with_wiki_sentences_eval(\n",
        "    mapping: Dict[str, Dict[int, str]],\n",
        "    df: pd.DataFrame,\n",
        "    is_testset: bool = False,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Only for creating dev and test sentences.\"\"\"\n",
        "    claims = []\n",
        "    sentences = []\n",
        "    evidence = []\n",
        "    predicted_evidence = []\n",
        "\n",
        "    # negative\n",
        "    for i in range(len(df)):\n",
        "        # if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
        "        #     continue\n",
        "        claim = df[\"claim\"].iloc[i]\n",
        "\n",
        "        predicted_pages = df[\"predicted_pages\"][i]\n",
        "        for page in predicted_pages:\n",
        "            page = page.replace(\" \", \"_\")\n",
        "            try:\n",
        "                page_sent_id_pairs = [(page, k) for k in mapping[page]]\n",
        "            except KeyError:\n",
        "                # print(f\"{page} is not in our Wiki db.\")\n",
        "                continue\n",
        "\n",
        "            for page_name, sentence_id in page_sent_id_pairs:\n",
        "                text = mapping[page][sentence_id]\n",
        "                if text != \"\":\n",
        "                    claims.append(claim)\n",
        "                    sentences.append(text)\n",
        "                    if not is_testset:\n",
        "                        evidence.append(df[\"evidence\"].iloc[i])\n",
        "                    predicted_evidence.append([page_name, int(sentence_id)])\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"claim\": claims,\n",
        "        \"text\": sentences,\n",
        "        \"evidence\": evidence if not is_testset else None,\n",
        "        \"predicted_evidence\": predicted_evidence,\n",
        "    })"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1. Setup training environment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title  { display-mode: \"form\" }\n",
        "\n",
        "MODEL_NAME = \"bert-base-chinese\"  #@param {type:\"string\"}\n",
        "NUM_EPOCHS = 1  #@param {type:\"integer\"}\n",
        "LR = 2e-5  #@param {type:\"number\"}\n",
        "TRAIN_BATCH_SIZE = 64  #@param {type:\"integer\"} 如果CUDA out of memory 就調小(通常為2的次方)\n",
        "TEST_BATCH_SIZE = 256  #@param {type:\"integer\"}\n",
        "NEGATIVE_RATIO = 0.03  #@param {type:\"number\"}\n",
        "VALIDATION_STEP = 50  #@param {type:\"integer\"}\n",
        "TOP_N = 5  #@param {type:\"integer\"} 輸出前五筆，與doc retrieval 同樣設定"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Experiment Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ToLvE9oxIXQo"
      },
      "outputs": [],
      "source": [
        "# 一些資料夾存放位置而已\n",
        "EXP_DIR = f\"sent_retrieval/e{NUM_EPOCHS}_bs{TRAIN_BATCH_SIZE}_\" + f\"{LR}_neg{NEGATIVE_RATIO}_top{TOP_N}\"\n",
        "LOG_DIR = \"logs/\" + EXP_DIR\n",
        "CKPT_DIR = \"checkpoints/\" + EXP_DIR\n",
        "\n",
        "if not Path(LOG_DIR).exists():\n",
        "    Path(LOG_DIR).mkdir(parents=True)\n",
        "\n",
        "if not Path(CKPT_DIR).exists():\n",
        "    Path(CKPT_DIR).mkdir(parents=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2. Combine claims and evidences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "4A5vWEzPiXGF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now using the following train data with 0 (Negative) and 1 (Positive)\n",
            "1    2774\n",
            "0    2643\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "train_df = pair_with_wiki_sentences(\n",
        "    mapping, # 就是把wiki_pages資料夾內的資料整理成一個字典{頁面名稱: {index: 句子}}\n",
        "    pd.DataFrame(TRAIN_GT), # train data\n",
        "    NEGATIVE_RATIO, # 自訂的threshold\n",
        ")\n",
        "counts = train_df[\"label\"].value_counts() # \n",
        "print(\"Now using the following train data with 0 (Negative) and 1 (Positive)\")\n",
        "print(counts)\n",
        "\n",
        "dev_evidences = pair_with_wiki_sentences_eval(mapping, pd.DataFrame(DEV_GT))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3. Start training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataloader things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "l48WifjeIGui"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbf52eaa39894a9d89dffc3167ef5d64",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Maggie\\miniconda3\\envs\\NLP_final\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Maggie\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d741ce17e15f43078366ef9bbbf0a185",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97923cbeadb94653b4885fa88e992939",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87f62a239a3d4aa4a347f1723d17e60e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "train_dataset = SentRetrievalBERTDataset(train_df, tokenizer=tokenizer)\n",
        "val_dataset = SentRetrievalBERTDataset(dev_evidences, tokenizer=tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        ")\n",
        "eval_dataloader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save your memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "del train_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "4rl_u0YbeQtY"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1c5dac1ddaa44f4b0e1ec7e83830add",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/412M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
        "    \"cpu\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
        "lr_scheduler = set_lr_scheduler(optimizer, num_training_steps)\n",
        "\n",
        "writer = SummaryWriter(LOG_DIR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please make sure that you are using gpu when training (5 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "1AHGaKh1eKmg"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70edd3d1890b4203ac1158289b68f089",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/85 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Maggie\\Documents\\Git_Workspace\\2023_spring_NLP\\NLP_final_project\\NCKU-AICUP2023-baseline\\full_pipeline.ipynb Cell 72\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Maggie/Documents/Git_Workspace/2023_spring_NLP/NLP_final_project/NCKU-AICUP2023-baseline/full_pipeline.ipynb#Y130sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maggie/Documents/Git_Workspace/2023_spring_NLP/NLP_final_project/NCKU-AICUP2023-baseline/full_pipeline.ipynb#Y130sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Maggie/Documents/Git_Workspace/2023_spring_NLP/NLP_final_project/NCKU-AICUP2023-baseline/full_pipeline.ipynb#Y130sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maggie/Documents/Git_Workspace/2023_spring_NLP/NLP_final_project/NCKU-AICUP2023-baseline/full_pipeline.ipynb#Y130sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maggie/Documents/Git_Workspace/2023_spring_NLP/NLP_final_project/NCKU-AICUP2023-baseline/full_pipeline.ipynb#Y130sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[1;32mc:\\Users\\Maggie\\miniconda3\\envs\\NLP_final\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Maggie\\miniconda3\\envs\\NLP_final\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "progress_bar = tqdm(range(num_training_steps))\n",
        "current_steps = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train() # 設定為 training 模式\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward() # backpropagation\n",
        "\n",
        "        optimizer.step() # 更新 weights\n",
        "        lr_scheduler.step() # 更新 weights\n",
        "        optimizer.zero_grad() # 將梯度歸零(清空 gradient buffer)\n",
        "        progress_bar.update(1)\n",
        "        writer.add_scalar(\"training_loss\", loss.item(), current_steps)\n",
        "\n",
        "        # 找出機率最大的label (0, 1)\n",
        "        y_pred = torch.argmax(outputs.logits, dim=1).tolist()\n",
        "        y_true = batch[\"labels\"].tolist()\n",
        "\n",
        "        current_steps += 1\n",
        "\n",
        "        # 到達一定的step就會output到tensorboard\n",
        "        if current_steps % VALIDATION_STEP == 0 and current_steps > 0:\n",
        "            print(\"Start validation\")\n",
        "            probs = get_predicted_probs(model, eval_dataloader, device)\n",
        "\n",
        "            val_results = evaluate_retrieval(\n",
        "                probs=probs,\n",
        "                df_evidences=dev_evidences,\n",
        "                ground_truths=DEV_GT,\n",
        "                top_n=TOP_N,\n",
        "            )\n",
        "            print(val_results)\n",
        "\n",
        "            # log each metric separately to TensorBoard\n",
        "            for metric_name, metric_value in val_results.items():\n",
        "                writer.add_scalar(\n",
        "                    f\"dev_{metric_name}\",\n",
        "                    metric_value,\n",
        "                    current_steps,\n",
        "                )\n",
        "\n",
        "            save_checkpoint(model, CKPT_DIR, current_steps)\n",
        "\n",
        "print(\"Finished training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validation part (15 mins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS1Ei5DAIO5p"
      },
      "outputs": [],
      "source": [
        "ckpt_name = \"model.50.pt\"  #@param {type:\"string\"}\n",
        "model = load_model(model, ckpt_name, CKPT_DIR)\n",
        "print(\"Start final evaluations and write prediction files.\")\n",
        "\n",
        "# 取得 wiki sentences\n",
        "train_evidences = pair_with_wiki_sentences_eval(\n",
        "    mapping=mapping,\n",
        "    df=pd.DataFrame(TRAIN_GT),\n",
        ")\n",
        "train_set = SentRetrievalBERTDataset(train_evidences, tokenizer)\n",
        "train_dataloader = DataLoader(train_set, batch_size=TEST_BATCH_SIZE)\n",
        "\n",
        "print(\"Start calculating training scores\")\n",
        "probs = get_predicted_probs(model, train_dataloader, device)\n",
        "train_results = evaluate_retrieval(\n",
        "    probs=probs,\n",
        "    df_evidences=train_evidences,\n",
        "    ground_truths=TRAIN_GT,\n",
        "    top_n=TOP_N,\n",
        "    save_name=f\"train_doc5sent{TOP_N}.jsonl\",\n",
        ")\n",
        "print(f\"Training scores => {train_results}\")\n",
        "\n",
        "print(\"Start validation\")\n",
        "probs = get_predicted_probs(model, eval_dataloader, device)\n",
        "val_results = evaluate_retrieval(\n",
        "    probs=probs,\n",
        "    df_evidences=dev_evidences,\n",
        "    ground_truths=DEV_GT,\n",
        "    top_n=TOP_N,\n",
        "    save_name=f\"dev_doc5sent{TOP_N}.jsonl\",\n",
        ")\n",
        "\n",
        "print(f\"Validation scores => {val_results}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4. Check on our test data\n",
        "(5 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVFusJqjmex-"
      },
      "outputs": [],
      "source": [
        "test_data = load_json(\"data/test_doc5.jsonl\")\n",
        "\n",
        "test_evidences = pair_with_wiki_sentences_eval(\n",
        "    mapping,\n",
        "    pd.DataFrame(test_data),\n",
        "    is_testset=True,\n",
        ")\n",
        "test_set = SentRetrievalBERTDataset(test_evidences, tokenizer)\n",
        "test_dataloader = DataLoader(test_set, batch_size=TEST_BATCH_SIZE)\n",
        "\n",
        "print(\"Start predicting the test data\")\n",
        "probs = get_predicted_probs(model, test_dataloader, device)\n",
        "evaluate_retrieval(\n",
        "    probs=probs,\n",
        "    df_evidences=test_evidences,\n",
        "    ground_truths=test_data,\n",
        "    top_n=TOP_N,\n",
        "    cal_scores=False,\n",
        "    save_name=f\"test_doc5sent{TOP_N}.jsonl\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lGzl8a5JteT7"
      },
      "source": [
        "notebook3\n",
        "## PART 3. Claim verification"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgA1vcUyzjlx"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandarallel import pandarallel\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    get_scheduler,\n",
        ")\n",
        "\n",
        "from dataset import BERTDataset\n",
        "from utils import (\n",
        "    generate_evidence_to_wiki_pages_mapping,\n",
        "    jsonl_dir_to_df,\n",
        "    load_json,\n",
        "    load_model,\n",
        "    save_checkpoint,\n",
        "    set_lr_scheduler,\n",
        ")\n",
        "\n",
        "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LABEL2ID: Dict[str, int] = {\n",
        "    \"supports\": 0,\n",
        "    \"refutes\": 1,\n",
        "    \"NOT ENOUGH INFO\": 2,\n",
        "}\n",
        "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
        "\n",
        "TRAIN_DATA = load_json(\"data/train_doc5sent5.jsonl\")\n",
        "DEV_DATA = load_json(\"data/dev_doc5sent5.jsonl\")\n",
        "\n",
        "TRAIN_PKL_FILE = Path(\"data/train_doc5sent5.pkl\")\n",
        "DEV_PKL_FILE = Path(\"data/dev_doc5sent5.pkl\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preload wiki database (same as part 2.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
        "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages,)\n",
        "del wiki_pages"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper function"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AICUP dataset with top-k evidence sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AicupTopkEvidenceBERTDataset(BERTDataset):\n",
        "    \"\"\"AICUP dataset with top-k evidence sentences.\"\"\"\n",
        "\n",
        "    def __getitem__(\n",
        "        self,\n",
        "        idx: int,\n",
        "        **kwargs,\n",
        "    ) -> Tuple[Dict[str, torch.Tensor], int]:\n",
        "        item = self.data.iloc[idx]\n",
        "        claim = item[\"claim\"]\n",
        "        evidence = item[\"evidence_list\"]\n",
        "\n",
        "        # In case there are less than topk evidence sentences\n",
        "        pad = [\"[PAD]\"] * (self.topk - len(evidence))\n",
        "        evidence += pad\n",
        "        concat_claim_evidence = \" [SEP] \".join([*claim, *evidence])\n",
        "\n",
        "        concat = self.tokenizer(\n",
        "            concat_claim_evidence,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        label = LABEL2ID[item[\"label\"]] if \"label\" in item else -1\n",
        "        concat_ten = {k: torch.tensor(v) for k, v in concat.items()}\n",
        "\n",
        "        if \"label\" in item:\n",
        "            concat_ten[\"labels\"] = torch.tensor(label)\n",
        "\n",
        "        return concat_ten"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_evaluation(model: torch.nn.Module, dataloader: DataLoader, device):\n",
        "    model.eval()\n",
        "\n",
        "    loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            y_true.extend(batch[\"labels\"].tolist())\n",
        "\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss += outputs.loss.item()\n",
        "            logits = outputs.logits\n",
        "            y_pred.extend(torch.argmax(logits, dim=1).tolist())\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    return {\"val_loss\": loss / len(dataloader), \"val_acc\": acc}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_predict(model: torch.nn.Module, test_dl: DataLoader, device) -> list:\n",
        "    model.eval()\n",
        "\n",
        "    preds = []\n",
        "    for batch in tqdm(test_dl,\n",
        "                      total=len(test_dl),\n",
        "                      leave=False,\n",
        "                      desc=\"Predicting\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        pred = model(**batch).logits\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        preds.extend(pred.tolist())\n",
        "    return preds"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def join_with_topk_evidence(\n",
        "    df: pd.DataFrame,\n",
        "    mapping: dict,\n",
        "    mode: str = \"train\",\n",
        "    topk: int = 5,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"join_with_topk_evidence join the dataset with topk evidence.\n",
        "\n",
        "    Note:\n",
        "        After extraction, the dataset will be like this:\n",
        "               id     label         claim                           evidence            evidence_list\n",
        "        0    4604  supports       高行健...     [[[3393, 3552, 高行健, 0], [...  [高行健 （ ）江西赣州出...\n",
        "        ..    ...       ...            ...                                ...                     ...\n",
        "        945  2095  supports       美國總...  [[[1879, 2032, 吉米·卡特, 16], [...  [卸任后 ， 卡特積極參與...\n",
        "        停各种战争及人質危機的斡旋工作 ， 反对美国小布什政府攻打伊拉克...\n",
        "\n",
        "        [946 rows x 5 columns]\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataset with evidence.\n",
        "        wiki_pages (pd.DataFrame): The wiki pages dataframe\n",
        "        topk (int, optional): The topk evidence. Defaults to 5.\n",
        "        cache(Union[Path, str], optional): The cache file path. Defaults to None.\n",
        "            If cache is None, return the result directly.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The dataset with topk evidence_list.\n",
        "            The `evidence_list` column will be: List[str]\n",
        "    \"\"\"\n",
        "\n",
        "    # format evidence column to List[List[Tuple[str, str, str, str]]]\n",
        "    if \"evidence\" in df.columns:\n",
        "        df[\"evidence\"] = df[\"evidence\"].parallel_map(\n",
        "            lambda x: [[x]] if not isinstance(x[0], list) else [x]\n",
        "            if not isinstance(x[0][0], list) else x)\n",
        "\n",
        "    print(f\"Extracting evidence_list for the {mode} mode ...\")\n",
        "    if mode == \"eval\":\n",
        "        # extract evidence\n",
        "        df[\"evidence_list\"] = df[\"predicted_evidence\"].parallel_map(lambda x: [\n",
        "            mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "            for evi_id, evi_idx in x  # for each evidence list\n",
        "        ][:topk] if isinstance(x, list) else [])\n",
        "        print(df[\"evidence_list\"][:5])\n",
        "    else:\n",
        "        # extract evidence\n",
        "        df[\"evidence_list\"] = df[\"evidence\"].parallel_map(lambda x: [\n",
        "            \" \".join([  # join evidence\n",
        "                mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "                for _, _, evi_id, evi_idx in evi_list\n",
        "            ]) if isinstance(evi_list, list) else \"\"\n",
        "            for evi_list in x  # for each evidence list\n",
        "        ][:1] if isinstance(x, list) else [])\n",
        "\n",
        "    return df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1. Setup training environment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title  { display-mode: \"form\" }\n",
        "\n",
        "MODEL_NAME = \"bert-base-chinese\"  #@param {type:\"string\"}\n",
        "TRAIN_BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
        "TEST_BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
        "SEED = 42  #@param {type:\"integer\"}\n",
        "LR = 7e-5  #@param {type:\"number\"}\n",
        "NUM_EPOCHS = 20  #@param {type:\"integer\"}\n",
        "MAX_SEQ_LEN = 256  #@param {type:\"integer\"}\n",
        "EVIDENCE_TOPK = 5  #@param {type:\"integer\"}\n",
        "VALIDATION_STEP = 25  #@param {type:\"integer\"}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Experiment Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_FILENAME = \"submission.jsonl\"\n",
        "\n",
        "EXP_DIR = f\"claim_verification/e{NUM_EPOCHS}_bs{TRAIN_BATCH_SIZE}_\" + f\"{LR}_top{EVIDENCE_TOPK}\"\n",
        "LOG_DIR = \"logs/\" + EXP_DIR\n",
        "CKPT_DIR = \"checkpoints/\" + EXP_DIR\n",
        "\n",
        "if not Path(LOG_DIR).exists():\n",
        "    Path(LOG_DIR).mkdir(parents=True)\n",
        "\n",
        "if not Path(CKPT_DIR).exists():\n",
        "    Path(CKPT_DIR).mkdir(parents=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2. Concat claim and evidences\n",
        "join topk evidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not TRAIN_PKL_FILE.exists():\n",
        "    train_df = join_with_topk_evidence(\n",
        "        pd.DataFrame(TRAIN_DATA),\n",
        "        mapping,\n",
        "        topk=EVIDENCE_TOPK,\n",
        "    )\n",
        "    train_df.to_pickle(TRAIN_PKL_FILE, protocol=4)\n",
        "else:\n",
        "    with open(TRAIN_PKL_FILE, \"rb\") as f:\n",
        "        train_df = pickle.load(f)\n",
        "\n",
        "if not DEV_PKL_FILE.exists():\n",
        "    dev_df = join_with_topk_evidence(\n",
        "        pd.DataFrame(DEV_DATA),\n",
        "        mapping,\n",
        "        mode=\"eval\",\n",
        "        topk=EVIDENCE_TOPK,\n",
        "    )\n",
        "    dev_df.to_pickle(DEV_PKL_FILE, protocol=4)\n",
        "else:\n",
        "    with open(DEV_PKL_FILE, \"rb\") as f:\n",
        "        dev_df = pickle.load(f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3. Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prevent CUDA out of memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0rVk3990DlD"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "train_dataset = AicupTopkEvidenceBERTDataset(\n",
        "    train_df,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_SEQ_LEN,\n",
        ")\n",
        "val_dataset = AicupTopkEvidenceBERTDataset(\n",
        "    dev_df,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_SEQ_LEN,\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        ")\n",
        "eval_dataloader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzMgs-Zs3sTN"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
        "    \"cpu\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(LABEL2ID),\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
        "lr_scheduler = set_lr_scheduler(optimizer, num_training_steps)\n",
        "\n",
        "writer = SummaryWriter(LOG_DIR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training (30 mins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aqMjEek3wmu"
      },
      "outputs": [],
      "source": [
        "progress_bar = tqdm(range(num_training_steps))\n",
        "current_steps = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "        writer.add_scalar(\"training_loss\", loss.item(), current_steps)\n",
        "\n",
        "        y_pred = torch.argmax(outputs.logits, dim=1).tolist()\n",
        "        y_true = batch[\"labels\"].tolist()\n",
        "\n",
        "        current_steps += 1\n",
        "\n",
        "        if current_steps % VALIDATION_STEP == 0 and current_steps > 0:\n",
        "            print(\"Start validation\")\n",
        "            val_results = run_evaluation(model, eval_dataloader, device)\n",
        "\n",
        "            # log each metric separately to TensorBoard\n",
        "            for metric_name, metric_value in val_results.items():\n",
        "                print(f\"{metric_name}: {metric_value}\")\n",
        "                writer.add_scalar(f\"{metric_name}\", metric_value, current_steps)\n",
        "\n",
        "            save_checkpoint(\n",
        "                model,\n",
        "                CKPT_DIR,\n",
        "                current_steps,\n",
        "                mark=f\"val_acc={val_results['val_acc']:.4f}\",\n",
        "            )\n",
        "\n",
        "print(\"Finished training!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4. Make your submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLkfuoAE49mz"
      },
      "outputs": [],
      "source": [
        "TEST_DATA = load_json(\"data/test_doc5sent5.jsonl\")\n",
        "TEST_PKL_FILE = Path(\"data/test_doc5sent5.pkl\")\n",
        "\n",
        "if not TEST_PKL_FILE.exists():\n",
        "    test_df = join_with_topk_evidence(\n",
        "        pd.DataFrame(TEST_DATA),\n",
        "        mapping,\n",
        "        mode=\"eval\",\n",
        "        topk=EVIDENCE_TOPK,\n",
        "    )\n",
        "    test_df.to_pickle(TEST_PKL_FILE, protocol=4)\n",
        "else:\n",
        "    with open(TEST_PKL_FILE, \"rb\") as f:\n",
        "        test_df = pickle.load(f)\n",
        "\n",
        "test_dataset = AicupTopkEvidenceBERTDataset(\n",
        "    test_df,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_SEQ_LEN,\n",
        ")\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqIjlht8yCMA"
      },
      "outputs": [],
      "source": [
        "ckpt_name = \"val_acc=0.4259_model.750.pt\"  #@param {type:\"string\"}\n",
        "model = load_model(model, ckpt_name, CKPT_DIR)\n",
        "predicted_label = run_predict(model, test_dataloader, device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl9I3ZWW4pHo"
      },
      "outputs": [],
      "source": [
        "predict_dataset = test_df.copy()\n",
        "predict_dataset[\"predicted_label\"] = list(map(ID2LABEL.get, predicted_label))\n",
        "predict_dataset[[\"id\", \"predicted_label\", \"predicted_evidence\"]].to_json(\n",
        "    OUTPUT_FILENAME,\n",
        "    orient=\"records\",\n",
        "    lines=True,\n",
        "    force_ascii=False,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "10286f3c74912972f7d1fdceceee5be5b7c77248e5efe5afcbc6a71f24d230fa"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
